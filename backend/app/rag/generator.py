from langchain_ollama import ChatOllama
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser
from app.config import OLLAMA_BASE_URL, LLM_MODEL, LLM_TEMPERATURE

# T羹rk癟e mevzuat asistan覺 i癟in 繹zelletirilmi prompt
RAG_PROMPT_TEMPLATE = """
Sen yard覺mc覺 bir hukuk asistan覺s覺n. Verilen balam覺 (context) kullanarak soruyu cevapla.

Kurallar:
1. Sadece verilen metne sad覺k kal, ancak metindeki anlam覺 birletirerek a癟覺klay覺c覺 ol.
2. Eer metinde tam cevap yoksa, "Verilen belgelerde bu bilgi dorudan yer alm覺yor ancak unlardan bahsediliyor..." diyerek elindeki bilgiyi 繹zetle.
3. Asla tamamen uydurma cevap verme.
4. M羹mk羹nse hangi kanun veya y繹netmelikten al覺nt覺 yapt覺覺n覺 belirt.

Balam:
{context}

Soru:
{question}

Cevap:
"""


def get_llm():
    """
    Ollama LLM instance'覺 d繹nd羹r羹r.
    """
    print(f" Ollama'ya balan覺l覺yor: {OLLAMA_BASE_URL}")

    llm = ChatOllama(
        model=LLM_MODEL,
        temperature=LLM_TEMPERATURE,
        base_url=OLLAMA_BASE_URL
    )

    return llm


def get_prompt():
    """
    RAG i癟in prompt template d繹nd羹r羹r.
    """
    return ChatPromptTemplate.from_template(RAG_PROMPT_TEMPLATE)


def generate_response(context: str, question: str) -> str:
    """
    Verilen context ve soru i癟in LLM'den cevap 羹retir.

    Args:
        context: 襤lgili dok羹man metinleri
        question: Kullan覺c覺 sorusu

    Returns:
        LLM'in 羹rettii cevap
    """
    llm = get_llm()
    prompt = get_prompt()
    output_parser = StrOutputParser()

    chain = prompt | llm | output_parser

    response = chain.invoke({
        "context": context,
        "question": question
    })

    return response
