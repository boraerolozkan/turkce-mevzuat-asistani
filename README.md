# TÃ¼rkÃ§e Mevzuat AsistanÄ± ğŸ›ï¸

[![Python](https://img.shields.io/badge/Python-3.11+-blue.svg)](https://www.python.org/)
[![FastAPI](https://img.shields.io/badge/FastAPI-0.100+-green.svg)](https://fastapi.tiangolo.com/)
[![React](https://img.shields.io/badge/React-19-blue.svg)](https://reactjs.org/)
[![LangChain](https://img.shields.io/badge/LangChain-Latest-orange.svg)](https://langchain.com/)
[![License](https://img.shields.io/badge/License-MIT-yellow.svg)](LICENSE)

A **Retrieval-Augmented Generation (RAG)** powered AI assistant for Turkish legislation. Ask questions about Turkish laws and regulations in natural language and get accurate, source-cited answers.

[ğŸ‡¹ğŸ‡· TÃ¼rkÃ§e](#tÃ¼rkÃ§e) | [ğŸ‡¬ğŸ‡§ English](#english)

---

## ğŸ¯ Demo

<div align="center">
  <img src="docs/demo.gif" alt="Demo" width="800"/>
</div>

**Live Demo:** [Coming Soon]

---

## âœ¨ Key Features

- ğŸ” **Semantic Search** - Find relevant legal provisions using natural language queries
- ğŸ“š **1000+ Legal Documents** - Covers Turkish laws, regulations, and bylaws
- ğŸ¯ **Source Citations** - Every answer includes references to specific legal documents
- ğŸ‡¹ğŸ‡· **Turkish-Optimized** - Uses multilingual embeddings tuned for Turkish language
- ğŸ’¬ **Conversational UI** - ChatGPT-style interface for intuitive interaction
- âš¡ **Fast Response** - Sub-3 second response time with optimized retrieval
- ğŸ³ **Docker Ready** - One-command deployment with Docker Compose

---

## ğŸ—ï¸ Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                           USER INTERFACE                            â”‚
â”‚                    React + Vite (Port 5173)                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                  â”‚ HTTP/REST
                                  â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                           BACKEND API                               â”‚
â”‚                    FastAPI + LangChain (Port 8000)                  â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                  â”‚
â”‚  â”‚   Routers   â”‚  â”‚     RAG     â”‚  â”‚  Ingestion  â”‚                  â”‚
â”‚  â”‚  /api/chat  â”‚  â”‚  Pipeline   â”‚  â”‚   Pipeline  â”‚                  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚                â”‚                â”‚
         â–¼                â–¼                â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Ollama    â”‚  â”‚   Qdrant    â”‚  â”‚  PDF Documents  â”‚
â”‚  (LLM)      â”‚  â”‚  (Vectors)  â”‚  â”‚  (1000+ files)  â”‚
â”‚ Port 11434  â”‚  â”‚  Port 6333  â”‚  â”‚  /data/mevzuat  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### RAG Pipeline Flow

```
User Question
      â”‚
      â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 1. Embedding    â”‚  Convert question to vector using
â”‚    Model        â”‚  sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 2. Vector       â”‚  Search Qdrant for top-k most similar
â”‚    Search       â”‚  document chunks (default k=5)
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 3. Context      â”‚  Combine retrieved chunks into
â”‚    Building     â”‚  a coherent context
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 4. LLM          â”‚  Generate answer using Llama 3.1
â”‚    Generation   â”‚  with context and question
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
   Answer + Sources
```

---

## ğŸ› ï¸ Tech Stack

| Layer | Technology | Purpose |
|-------|------------|---------|
| **Frontend** | React 19, Vite, Axios | User interface |
| **Backend** | FastAPI, Python 3.11+ | REST API |
| **LLM Orchestration** | LangChain | RAG pipeline management |
| **Vector Database** | Qdrant | Semantic similarity search |
| **Embeddings** | sentence-transformers | Text-to-vector conversion |
| **LLM** | Ollama (Llama 3.1) | Response generation |
| **Containerization** | Docker, Docker Compose | Deployment |

---

## ğŸ“ Project Structure

```
turkce-mevzuat-asistani/
â”œâ”€â”€ backend/
â”‚   â”œâ”€â”€ app/
â”‚   â”‚   â”œâ”€â”€ main.py              # FastAPI application entry
â”‚   â”‚   â”œâ”€â”€ config.py            # Environment configuration
â”‚   â”‚   â”œâ”€â”€ models.py            # Pydantic schemas
â”‚   â”‚   â”œâ”€â”€ rag/
â”‚   â”‚   â”‚   â”œâ”€â”€ embeddings.py    # Embedding model management
â”‚   â”‚   â”‚   â”œâ”€â”€ retriever.py     # Qdrant search operations
â”‚   â”‚   â”‚   â”œâ”€â”€ generator.py     # LLM response generation
â”‚   â”‚   â”‚   â””â”€â”€ pipeline.py      # Full RAG chain
â”‚   â”‚   â”œâ”€â”€ ingestion/
â”‚   â”‚   â”‚   â”œâ”€â”€ pdf_loader.py    # PDF document loading
â”‚   â”‚   â”‚   â”œâ”€â”€ chunker.py       # Text chunking logic
â”‚   â”‚   â”‚   â””â”€â”€ indexer.py       # Qdrant indexing
â”‚   â”‚   â””â”€â”€ routers/
â”‚   â”‚       â””â”€â”€ chat.py          # Chat API endpoints
â”‚   â”œâ”€â”€ requirements.txt
â”‚   â””â”€â”€ Dockerfile
â”œâ”€â”€ frontend/
â”‚   â”œâ”€â”€ src/
â”‚   â”‚   â”œâ”€â”€ components/
â”‚   â”‚   â”‚   â”œâ”€â”€ ChatWindow.jsx   # Main chat interface
â”‚   â”‚   â”‚   â”œâ”€â”€ MessageBubble.jsx
â”‚   â”‚   â”‚   â””â”€â”€ SourceCard.jsx   # Source citation display
â”‚   â”‚   â”œâ”€â”€ hooks/
â”‚   â”‚   â”‚   â””â”€â”€ useChat.js       # Chat state management
â”‚   â”‚   â”œâ”€â”€ services/
â”‚   â”‚   â”‚   â””â”€â”€ api.js           # Backend communication
â”‚   â”‚   â””â”€â”€ App.jsx
â”‚   â”œâ”€â”€ package.json
â”‚   â””â”€â”€ Dockerfile
â”œâ”€â”€ data/
â”‚   â””â”€â”€ mevzuat/                 # PDF files (not in repo)
â”œâ”€â”€ docker-compose.yml
â””â”€â”€ README.md
```

---

## ğŸš€ Quick Start

### Prerequisites

- Docker & Docker Compose
- [Ollama](https://ollama.com/) installed locally
- 8GB+ RAM recommended

### 1. Clone the Repository

```bash
git clone https://github.com/YOUR_USERNAME/turkce-mevzuat-asistani.git
cd turkce-mevzuat-asistani
```

### 2. Setup Ollama

```bash
# Install Ollama (if not installed)
curl -fsSL https://ollama.com/install.sh | sh

# Pull the Llama 3.1 model
ollama pull llama3.1

# Start Ollama server
ollama serve
```

### 3. Download Legal Documents

Download Turkish legislation PDFs and place them in `data/mevzuat/`:

```bash
mkdir -p data/mevzuat
# Add your PDF files to this directory
# You can scrape from mevzuat.gov.tr or use the provided scraper
python data/data_scraping.py
```

### 4. Configure Environment

Create a `.env` file or update `docker-compose.yml`:

```env
OLLAMA_BASE_URL=http://host.docker.internal:11434  # For Docker
# or
OLLAMA_BASE_URL=http://YOUR_LOCAL_IP:11434  # For network access
```

### 5. Start Services

```bash
docker-compose up -d
```

### 6. Index Documents (First Time Only)

```bash
docker exec -it turkce-mevzuat-asistani_backend_1 \
  python -m app.ingestion.ingest
```

### 7. Access the Application

- **Frontend:** http://localhost:5173
- **API Docs:** http://localhost:8000/docs
- **Qdrant Dashboard:** http://localhost:6333/dashboard

---

## ğŸ“– API Reference

### POST `/api/chat`

Send a question and receive an AI-generated answer with sources.

**Request:**
```json
{
  "question": "YÄ±llÄ±k izin sÃ¼resi kaÃ§ gÃ¼ndÃ¼r?"
}
```

**Response:**
```json
{
  "answer": "4857 sayÄ±lÄ± Ä°ÅŸ Kanunu'nun 53. maddesine gÃ¶re...",
  "sources": [
    {
      "source": "4857_is_kanunu.pdf",
      "page_content": "Madde 53 - Ä°ÅŸyerinde iÅŸe baÅŸladÄ±ÄŸÄ± gÃ¼nden itibaren...",
      "score": 0.89
    }
  ]
}
```

### GET `/`

Health check endpoint.

```json
{
  "status": "ok",
  "message": "Mevzuat AsistanÄ± API Ã‡alÄ±ÅŸÄ±yor ğŸš€"
}
```

---

## âš™ï¸ Configuration

| Variable | Default | Description |
|----------|---------|-------------|
| `QDRANT_URL` | `http://localhost:6333` | Qdrant server address |
| `COLLECTION_NAME` | `mevzuat_db` | Qdrant collection name |
| `OLLAMA_BASE_URL` | `http://localhost:11434` | Ollama server address |
| `LLM_MODEL` | `llama3.1` | LLM model to use |
| `LLM_TEMPERATURE` | `0` | Response randomness (0-1) |
| `RETRIEVER_K` | `5` | Number of documents to retrieve |
| `CHUNK_SIZE` | `800` | Document chunk size |
| `CHUNK_OVERLAP` | `100` | Overlap between chunks |
| `EMBEDDING_MODEL` | `sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2` | Embedding model |

---

## ğŸ§ª Development

### Backend Development

```bash
cd backend
python -m venv venv
source venv/bin/activate  # Windows: venv\Scripts\activate
pip install -r requirements.txt
uvicorn app.main:app --reload --port 8000
```

### Frontend Development

```bash
cd frontend
npm install
npm run dev
```

### Running Tests

```bash
# Backend tests
cd backend
pytest

# Frontend tests
cd frontend
npm test
```

---

## ğŸŒ Deployment

### Option 1: VPS Deployment (Recommended for Testing)

**Minimum Requirements:**
- 4 vCPU
- 8GB RAM
- 50GB SSD
- Ubuntu 22.04

**Recommended Providers:**
- Hetzner Cloud (CPX31) - ~â‚¬15/month
- DigitalOcean (4GB Droplet) - ~$24/month
- AWS EC2 (t3.large) - ~$60/month

**Setup:**
```bash
# Install Docker
curl -fsSL https://get.docker.com | sh

# Install Ollama
curl -fsSL https://ollama.com/install.sh | sh
ollama pull llama3.1

# Clone and start
git clone https://github.com/YOUR_USERNAME/turkce-mevzuat-asistani.git
cd turkce-mevzuat-asistani
docker-compose up -d
```

### Option 2: Cloud Run / Serverless (Not Recommended)

Due to Ollama requirements, serverless deployment is challenging. Consider using OpenAI API instead for serverless scenarios.

---

## ğŸ”’ Rate Limiting & Queue System

For production deployment with multiple users, implement request queuing:

### Redis + Celery Setup

```python
# backend/app/queue/tasks.py
from celery import Celery

celery_app = Celery('tasks', broker='redis://localhost:6379/0')

@celery_app.task
def process_chat_request(question: str):
    # RAG pipeline execution
    pass
```

### Docker Compose with Redis

```yaml
services:
  redis:
    image: redis:alpine
    ports:
      - "6379:6379"

  celery_worker:
    build: ./backend
    command: celery -A app.queue.tasks worker --loglevel=info
    depends_on:
      - redis
```

### Rate Limiting with FastAPI

```python
from slowapi import Limiter
from slowapi.util import get_remote_address

limiter = Limiter(key_func=get_remote_address)

@router.post("/chat")
@limiter.limit("10/minute")
async def chat_endpoint(request: Request, ...):
    pass
```

---

## ğŸ“Š Performance Metrics

| Metric | Value |
|--------|-------|
| Average Response Time | 2-4 seconds |
| Documents Indexed | 1,007 PDFs |
| Vector Dimensions | 384 |
| Chunk Size | 800 chars |
| Retrieval Accuracy | ~85% relevance |

---

## ğŸ¤ Contributing

Contributions are welcome! Please feel free to submit a Pull Request.

1. Fork the repository
2. Create your feature branch (`git checkout -b feature/AmazingFeature`)
3. Commit your changes (`git commit -m 'Add some AmazingFeature'`)
4. Push to the branch (`git push origin feature/AmazingFeature`)
5. Open a Pull Request

---

## ğŸ“„ License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

---

## ğŸ™ Acknowledgments

- [LangChain](https://langchain.com/) for the RAG framework
- [Qdrant](https://qdrant.tech/) for vector similarity search
- [Ollama](https://ollama.com/) for local LLM inference
- [FastAPI](https://fastapi.tiangolo.com/) for the backend framework
- Turkish Government's [mevzuat.gov.tr](https://mevzuat.gov.tr) for legal documents

---

## ğŸ“¬ Contact

**Bora Erol Ã–zkan**

- GitHub: [@YOUR_USERNAME](https://github.com/boraerolozkan)
- LinkedIn: [Your LinkedIn](https://linkedin.com/in/boraerolozkan)
- Email: boraerolozkan@gmail.com

---

<div align="center">
  <sub>Built with â¤ï¸ for the Turkish legal community</sub>
</div>
